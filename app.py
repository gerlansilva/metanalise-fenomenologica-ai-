import streamlit as st
import pandas as pd
import json
import os
from google import genai
from google.genai import types
from pydantic import BaseModel, Field

# Configura칞칚o da p치gina
st.set_page_config(page_title="Metan치lise Fenomenol칩gica AI", page_icon="游닀", layout="wide")

# Inicializa o cliente Gemini
# Certifique-se de configurar a vari치vel de ambiente GEMINI_API_KEY
api_key = os.environ.get("GEMINI_API_KEY")
if not api_key:
    st.warning("丘멆잺 Vari치vel de ambiente GEMINI_API_KEY n칚o encontrada. Por favor, configure-a para continuar.")
    st.stop()

client = genai.Client(api_key=api_key)

# --- Modelos Pydantic para Sa칤da Estruturada (Structured Output) ---

class UnidadeSentido(BaseModel):
    id_unidade: str = Field(description="ID 칰nico autom치tico, ex: DOC01_P087_US03")
    documento: str = Field(description="Nome do arquivo PDF")
    pagina: int | None = Field(description="N칰mero da p치gina onde o trecho aparece, null se n칚o encontrado")
    citacao_literal: str = Field(description="Trecho exato do texto, sem altera칞칫es")
    contexto_resumido: str | None = None
    justificativa_fenomenologica: str | None = None

class UnidadeSignificado(BaseModel):
    id_unidade: str
    documento: str
    trecho_original: str
    sintese: str

class Categoria(BaseModel):
    nome: str
    descricao: str
    unidades_relacionadas: list[str]

class PhenomenologicalResult(BaseModel):
    unidades_sentido: list[UnidadeSentido]
    unidades_significado: list[UnidadeSignificado]
    categorias: list[Categoria]

class SystematicAnswer(BaseModel):
    pergunta: str
    resposta: str
    evidencia_textual: str
    pagina: int | None = None

class SystematicDocument(BaseModel):
    documento: str
    respostas: list[SystematicAnswer]

class SystematicResult(BaseModel):
    documentos: list[SystematicDocument]

class AnalysisResult(BaseModel):
    fenomenologico: PhenomenologicalResult | None = None
    sistematico: SystematicResult | None = None

# --- Interface do Usu치rio (UI) ---

st.title("游닀 Metan치lise Fenomenol칩gica AI")
st.markdown("""
Fa칞a o upload de m칰ltiplos artigos em PDF e escolha o modo de an치lise. 
O sistema analisar치 os textos como um corpus 칰nico, extraindo unidades fenomenol칩gicas 
ou realizando um mapeamento sistem치tico.
""")

mode = st.radio(
    "Modo de An치lise",
    ["Fenomenol칩gico", "Mapeamento Sistem치tico", "Ambos"],
    horizontal=True,
    help="Fenomenol칩gico: Unidades de sentido, significado e categorias. Sistem치tico: Respostas objetivas a perguntas diretas."
)

phenom_q = ""
sys_q = ""

if mode in ["Fenomenol칩gico", "Ambos"]:
    phenom_q = st.text_area(
        "Interroga칞칚o Fenomenol칩gica",
        placeholder="Ex: Como o campo da Educa칞칚o Estat칤stica se constitui nos textos analisados?",
        height=100
    )

if mode in ["Mapeamento Sistem치tico", "Ambos"]:
    sys_q = st.text_area(
        "Perguntas para Mapeamento Sistem치tico",
        placeholder="1. Qual 칠 o objetivo do estudo?\n2. Qual metodologia 칠 utilizada?\n3. Qual referencial te칩rico?",
        height=150,
        help="Insira uma pergunta por linha."
    )

uploaded_files = st.file_uploader("Corpus Documental (PDFs)", type="pdf", accept_multiple_files=True)

if st.button("Iniciar An치lise do Corpus", type="primary", disabled=not uploaded_files):
    if mode in ["Fenomenol칩gico", "Ambos"] and not phenom_q.strip():
        st.warning("Por favor, preencha a Interroga칞칚o Fenomenol칩gica.")
        st.stop()
    if mode in ["Mapeamento Sistem치tico", "Ambos"] and not sys_q.strip():
        st.warning("Por favor, preencha as Perguntas para Mapeamento Sistem치tico.")
        st.stop()
        
    # Valida칞칚o de tamanho (limite de ~15MB para n칚o estourar 1M tokens)
    total_size = sum([f.size for f in uploaded_files])
    if total_size > 15 * 1024 * 1024:
        st.error(f"O tamanho total dos arquivos ({total_size / 1024 / 1024:.2f} MB) excede o limite seguro de 15 MB. Por favor, reduza o n칰mero de PDFs.")
        st.stop()

    with st.spinner("Analisando o corpus documental... Isso pode levar alguns minutos."):
        try:
            # Preparar arquivos para a API do Gemini
            gemini_files = []
            for file in uploaded_files:
                gemini_files.append(
                    types.Part.from_bytes(
                        data=file.getvalue(),
                        mime_type="application/pdf"
                    )
                )
            
            prompt_text = "Leia todos os PDFs anexados como um corpus 칰nico.\n\n"
            
            if mode in ["Fenomenol칩gico", "Ambos"]:
                prompt_text += "=== MODO FENOMENOL칍GICO ===\n"
                prompt_text += f"INTERROGA칂츾O FENOMENOL칍GICA:\n\"{phenom_q}\"\n\n"
                prompt_text += "Execute:\n"
                prompt_text += "ETAPA 1: Extraia unidades de sentido. Para cada unidade indique documento, p치gina, cita칞칚o literal exata, breve contexto e justificativa.\n"
                prompt_text += "Regras: N츾O parafrasear a cita칞칚o, N츾O inventar p치ginas, N츾O omitir documento, cada unidade deve ser rastre치vel.\n"
                prompt_text += "ETAPA 2: Transforme cada unidade em unidade de significado.\n"
                prompt_text += "ETAPA 3: Agrupe converg칡ncias entre documentos.\n"
                prompt_text += "ETAPA 4: Sugira categorias fenomenol칩gicas.\n\n"

            if mode in ["Mapeamento Sistem치tico", "Ambos"]:
                prompt_text += "=== MODO MAPEAMENTO SISTEM츼TICO ===\n"
                prompt_text += "Responda 맙 seguintes perguntas para CADA documento anexado:\n"
                prompt_text += f"{sys_q}\n\n"
                prompt_text += "Regras: Forne칞a respostas objetivas, cite a evid칡ncia textual exata e a p치gina onde foi encontrada.\n\n"

            contents = gemini_files + [prompt_text]

            # Selecionar o Schema correto baseado no modo
            schema = AnalysisResult
            if mode == "Fenomenol칩gico":
                schema = PhenomenologicalResult
            elif mode == "Mapeamento Sistem치tico":
                schema = SystematicResult

            # Chamada  API
            response = client.models.generate_content(
                model='gemini-2.5-pro', # Usando o modelo Pro mais recente
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction="""Voc칡 칠 um assistente de an치lise qualitativa de corpus documental.
Voc칡 analisar치 m칰ltiplos artigos cient칤ficos como um corpus 칰nico.
Siga estritamente as instru칞칫es do prompt e preencha o JSON de sa칤da corretamente.
Nunca invente conte칰do. Sempre preserve a rastreabilidade.
Se o n칰mero da p치gina n칚o puder ser identificado com certeza, retorne null para a p치gina.""",
                    response_mime_type="application/json",
                    response_schema=schema,
                    temperature=0.2
                ),
            )

            st.success("An치lise conclu칤da com sucesso!")
            
            # Parse da resposta JSON
            result_data = json.loads(response.text)
            
            st.header("Resultados da An치lise")
            
            tabs = []
            if mode in ["Fenomenol칩gico", "Ambos"]:
                tabs.extend(["Unidades de Sentido", "Unidades de Significado", "Categorias"])
            if mode in ["Mapeamento Sistem치tico", "Ambos"]:
                tabs.append("Mapeamento Sistem치tico")
                
            st_tabs = st.tabs(tabs)
            
            phenom_data = result_data if mode == "Fenomenol칩gico" else result_data.get("fenomenologico")
            sys_data = result_data if mode == "Mapeamento Sistem치tico" else result_data.get("sistematico")
            
            tab_idx = 0
            
            if mode in ["Fenomenol칩gico", "Ambos"] and phenom_data:
                # Aba 1: Unidades de Sentido
                with st_tabs[tab_idx]:
                    df_sentido = pd.DataFrame(phenom_data["unidades_sentido"])
                    st.dataframe(df_sentido, use_container_width=True)
                    csv = df_sentido.to_csv(index=False).encode('utf-8')
                    st.download_button("Baixar CSV (Unidades de Sentido)", csv, "unidades_sentido.csv", "text/csv")
                tab_idx += 1
                
                # Aba 2: Unidades de Significado
                with st_tabs[tab_idx]:
                    df_sig = pd.DataFrame(phenom_data["unidades_significado"])
                    st.dataframe(df_sig, use_container_width=True)
                tab_idx += 1
                
                # Aba 3: Categorias
                with st_tabs[tab_idx]:
                    for cat in phenom_data["categorias"]:
                        with st.expander(f"游늬 {cat['nome']}"):
                            st.write(cat['descricao'])
                            st.write("**Unidades Relacionadas:**", ", ".join(cat['unidades_relacionadas']))
                tab_idx += 1
                
            if mode in ["Mapeamento Sistem치tico", "Ambos"] and sys_data:
                # Aba 4: Mapeamento Sistem치tico (Formato Amplo / Wide Format)
                with st_tabs[tab_idx]:
                    docs = sys_data["documentos"]
                    
                    # Extrair perguntas 칰nicas para virarem colunas
                    unique_qs = []
                    for doc in docs:
                        for ans in doc["respostas"]:
                            if ans["pergunta"] not in unique_qs:
                                unique_qs.append(ans["pergunta"])
                                
                    rows = []
                    for doc in docs:
                        row = {"Documento": doc["documento"]}
                        for q in unique_qs:
                            ans_obj = next((a for a in doc["respostas"] if a["pergunta"] == q), None)
                            if ans_obj:
                                pag_str = f" (P치g. {ans_obj['pagina']})" if ans_obj.get('pagina') else ""
                                cell_val = f"Resposta: {ans_obj['resposta']}\n\nEvid칡ncia: \"{ans_obj['evidencia_textual']}\"{pag_str}"
                                row[q] = cell_val
                            else:
                                row[q] = "-"
                        rows.append(row)
                        
                    df_sys = pd.DataFrame(rows)
                    st.dataframe(df_sys, use_container_width=True)
                    
                    csv = df_sys.to_csv(index=False).encode('utf-8')
                    st.download_button("Baixar CSV (Mapeamento Sistem치tico)", csv, "mapeamento_sistematico.csv", "text/csv")

        except Exception as e:
            if "exceeds the maximum number of tokens allowed" in str(e):
                st.error("O corpus documental 칠 muito grande (excede o limite de tokens). Por favor, reduza a quantidade de PDFs.")
            else:
                st.error(f"Erro durante a an치lise: {e}")
